import streamlit as st
from openai import OpenAI
import time
import os

# from transformers import AutoTokenizer, AutoModelForCausalLM


# æ¸…é™¤å¯èƒ½å¯¼è‡´proxiesé”™è¯¯çš„ç’°å¢ƒå˜é‡
if 'http_proxy' in os.environ:
    del os.environ['http_proxy']
if 'https_proxy' in os.environ:
    del os.environ['https_proxy']

# é¡µé¢é…ç½®
# å°è¯•å¤šç§å›¾æ ‡è®¾ç½®æ–¹å¼ï¼Œæé«˜å…¼å®¹æ€§
try:
    # æ–¹æ³•1ï¼šä½¿ç”¨emojiå­—ç¬¦
    st.set_page_config(
        page_title="äººåŠ›èµ„æºåŠ©ç†å¤§æ¨¡å‹",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
except Exception as e:
    try:
        # æ–¹æ³•2ï¼šä½¿ç”¨åœ¨çº¿å›¾ç‰‡URL
        st.set_page_config(
            page_title="äººåŠ›èµ„æºåŠ©ç†å¤§æ¨¡å‹",
            page_icon="https://emoji.aranja.com/static/emoji-data/img-apple-160/1f916.png",
            layout="wide",
            initial_sidebar_state="expanded"
        )
    except Exception as e:
        # æ–¹æ³•3ï¼šä½¿ç”¨é»˜è®¤å›¾æ ‡
        st.set_page_config(
            page_title="äººåŠ›èµ„æºåŠ©ç†å¤§æ¨¡å‹",
            layout="wide",
            initial_sidebar_state="expanded"
        )

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

if "api_key" not in st.session_state:
    st.session_state.api_key = "sk-056383c136704516ad7f09b05f2e418c"

if "base_url" not in st.session_state:
    st.session_state.base_url = "https://api.deepseek.com"

if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "You are a professional Human Resources (HR) analysis assistant specialized in answering questions related to HR management.  \n\n**Workflow:**  \n\n1. **Determine Question Type**: First, analyze whether the user's question falls under the HR domain, including but not limited to:  \n   - Recruitment & Interviewing  \n   - Employee Training & Development  \n   - Compensation & Benefits  \n   - Performance Management  \n   - Employee Relations & Communication  \n   - Labor Law & Compliance  \n   - Organizational Development & Talent Management  \n\n2. **Non-HR Question Handling**: If the question is unrelated to HR (e.g., programming, math, entertainment), respond with:  \n   \"I am an HR analysis model. Please enter an HR-related question.\"  \n\n3. **HR Question Handling**: If the question is HR-related, provide a professional, clear, and practical answer."

# åˆå§‹åŒ–æ–‡ä»¶ä¸Šä¼ åŒºåŸŸæ˜¾ç¤ºçŠ¶æ€
if "show_file_uploader" not in st.session_state:
    st.session_state.show_file_uploader = False

# åˆå§‹åŒ–æ¨¡å‹ç±»å‹
if "model_type" not in st.session_state:
    st.session_state.model_type = "online"  # é»˜è®¤ä½¿ç”¨åœ¨çº¿æ¨¡å‹

# åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹å‚æ•°
if "local_model" not in st.session_state:
    st.session_state.local_model = "deepseek-r1:1.5b"  # é»˜è®¤æœ¬åœ°æ¨¡å‹

# ä¾§è¾¹æ  - å‚æ•°è®¾ç½®
with st.sidebar:
    st.title("âš™ï¸ è¶…å‚è®¾ç½®")

    # ç®€å†åˆ†ææŒ‰é’®
    if st.button("ğŸ“„ ç®€å†åˆ†æ"):
        st.session_state.show_file_uploader = not st.session_state.show_file_uploader
        st.rerun()

    # æ˜¾ç¤ºå½“å‰æ–‡ä»¶ä¸Šä¼ åŒºåŸŸçŠ¶æ€
    status = "æ˜¾ç¤º" if st.session_state.show_file_uploader else "éšè—"
    st.caption(f"æ–‡ä»¶ä¸Šä¼ åŒºåŸŸçŠ¶æ€: {status}")

    st.markdown("---")

    # æ¨¡å‹å‚æ•°è®¾ç½®
    st.session_state.max_tokens = st.slider(
        "ğŸ“ æœ€å¤§é•¿åº¦",
        min_value=1,
        max_value=32768,
        value=2048,
        step=1
    )

    st.session_state.temperature = st.slider(
        "ğŸŒ¡ï¸ é‡‡æ ·æ¸©åº¦",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.01
    )

    st.session_state.top_p = st.slider(
        "ğŸ“Š æ ¸é‡‡æ ·ç‡",
        min_value=0.0,
        max_value=1.0,
        value=0.9,
        step=0.01
    )

    # ç³»ç»Ÿæç¤ºè¯è®¾ç½®
    st.session_state.system_prompt = st.text_area(
        "ğŸ’¬ ç³»ç»Ÿæç¤º",
        value=st.session_state.system_prompt,
        height=100
    )

    # æ¨¡å‹ç±»å‹åˆ‡æ¢
    st.markdown("---")
    st.subheader("ğŸ”„ æ¨¡å‹ç±»å‹åˆ‡æ¢")
    model_type = st.radio(
        "é€‰æ‹©æ¨¡å‹ç±»å‹",
        options=["åœ¨çº¿æ¨¡å‹", "æœ¬åœ°æ¨¡å‹"],
        index=0 if st.session_state.model_type == "online" else 1,
        horizontal=True
    )

    # æ ¹æ®é€‰æ‹©æ›´æ–°æ¨¡å‹ç±»å‹
    if model_type == "åœ¨çº¿æ¨¡å‹":
        st.session_state.model_type = "online"
    else:
        st.session_state.model_type = "local"

    # APIè®¾ç½®
    with st.expander("ğŸ”‘ åœ¨çº¿æ¨¡å‹è®¾ç½®"):
        st.session_state.api_key = st.text_input(
            "API Key",
            value=st.session_state.api_key,
            type="password"
        )
        st.session_state.base_url = st.text_input(
            "API URL",
            value=st.session_state.base_url
        )
    # æœ¬åœ°æ¨¡å‹è®¾ç½®
    if st.session_state.model_type == "local":
        with st.expander("ğŸ”§ æœ¬åœ°æ¨¡å‹è®¾ç½®", expanded=True):
            st.session_state.local_model = st.selectbox(
                "æœ¬åœ°æ¨¡å‹",
                ["deepseek-r1:1.5b", "llama3.2:1b", "mistral", "mixtral", "qwen"],
                index=0
            )
            st.caption("ä½¿ç”¨Ollamaæä¾›çš„æœ¬åœ°æ¨¡å‹æœåŠ¡ï¼Œç¡®ä¿å·²å¯åŠ¨Ollamaå¹¶ä¸‹è½½ç›¸åº”æ¨¡å‹")

    st.markdown("---")
    # æ¸…ç©ºèŠå¤©æŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºèŠå¤©è®°å½•"):
        st.session_state.messages = []
        st.rerun()  # å°† experimental_rerun() æ›¿æ¢ä¸º rerun()

# ä¸»ç•Œé¢ - èŠå¤©åŒºåŸŸ
st.title("ğŸ¤– äººåŠ›èµ„æºåŠ©ç†å¤§æ¨¡å‹ ğŸ¤–")

# æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
col1, col2, col3 = st.columns(3)
with col1:
    st.caption("ğŸ” ç‰ˆæœ¬: äººåŠ›èµ„æºVer1.0")
with col2:
    st.caption("ğŸ‘¤ ä½œè€…: æä¼Ÿ")
with col3:
    st.caption("ğŸ“§ é‚®ç®±: 418472275@qq.com")

# æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹ç±»å‹
model_status = "ğŸŒ åœ¨çº¿æ¨¡å‹: DeepSeek" if st.session_state.model_type == "online" else f"ğŸ’» æœ¬åœ°æ¨¡å‹: {st.session_state.local_model}"
st.info(model_status)

st.markdown("---")
# å¯¹è¯åŒºåŸŸ
with st.chat_message("assistant"):
    st.markdown("ğŸ‘‹ ä½ å¥½ï¼Œæˆ‘æ˜¯äººåŠ›èµ„æºåŠ©ç†å¤§æ¨¡å‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ")

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
user_input = st.chat_input("è¯·è¾“å…¥äººåŠ›èµ„æºç±»é—®é¢˜")

# æ ¹æ®ä¼šè¯çŠ¶æ€æ¡ä»¶æ€§åœ°æ˜¾ç¤ºæ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
if st.session_state.show_file_uploader:
    # æ·»åŠ æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
    uploaded_file = st.file_uploader("ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼ˆå¯é€‰ï¼‰", type=["pdf", "docx", "txt", "xlsx"])

    # æ˜¾ç¤ºä¸Šä¼ æ–‡ä»¶ä¿¡æ¯
    if uploaded_file is not None:
        file_details = {
            "æ–‡ä»¶å": uploaded_file.name,
            "æ–‡ä»¶ç±»å‹": uploaded_file.type,
            "æ–‡ä»¶å¤§å°": f"{uploaded_file.size / 1024:.2f} KB"
        }
        st.write("**å·²ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯:**")
        for key, value in file_details.items():
            st.write(f"- {key}: {value}")

        # å°†æ–‡ä»¶ä¿¡æ¯æ·»åŠ åˆ°ç”¨æˆ·è¾“å…¥ä¸­
        if user_input:
            user_input += f"\n\n[ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶: {uploaded_file.name}]"
else:
    # å½“æ–‡ä»¶ä¸Šä¼ åŒºåŸŸéšè—æ—¶ï¼Œç¡®ä¿uploaded_fileå˜é‡å­˜åœ¨ä½†ä¸ºNone
    uploaded_file = None

# å¤„ç†ç”¨æˆ·è¾“å…¥
if user_input:
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.session_state.messages.append({"role": "user", "content": user_input})

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(user_input)

    # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯ï¼ˆå¸¦åŠ è½½åŠ¨ç”»ï¼‰
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        try:
            # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
            if st.session_state.model_type == "online":
                # ä½¿ç”¨åœ¨çº¿DeepSeekæ¨¡å‹
                client = OpenAI(
                    api_key=st.session_state.api_key,
                    base_url=st.session_state.base_url
                )
                model_name = "deepseek-chat"
            else:
                # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹
                try:
                    client = OpenAI(
                        api_key="ollama",  # ä»»æ„å€¼ï¼ŒOllamaä¸æ£€æŸ¥APIå¯†é’¥
                        base_url="http://localhost:11434/v1"
                    )
                    # å®šä¹‰æ¨¡å‹åç§° - ç¡®ä¿åœ¨tryå—å†…æå‰å®šä¹‰
                    model_name = st.session_state.local_model
                    # æµ‹è¯•è¿æ¥æ˜¯å¦æˆåŠŸ
                    client.models.list()

                except Exception as local_err:
                    st.error(f"è¿æ¥æœ¬åœ°OllamaæœåŠ¡å¤±è´¥: {str(local_err)}")
                    st.warning(
                        "è¯·ç¡®ä¿å·²å¯åŠ¨OllamaæœåŠ¡ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ http://localhost:11434 è®¿é—®ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å‹ã€‚")
                    message_placeholder.markdown(f"âŒ æœ¬åœ°æ¨¡å‹è¿æ¥é”™è¯¯: æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaå·²å¯åŠ¨å¹¶è¿è¡Œã€‚")
                    # return

            # æ³¨æ„ï¼šæ–°ç‰ˆæœ¬OpenAIåº“ä¸å†æ”¯æŒç›´æ¥ä¿®æ”¹proxieså±æ€§
            # å¦‚æœéœ€è¦è®¾ç½®ä»£ç†ï¼Œåº”åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®

            # å‡†å¤‡æ¶ˆæ¯å†å²
            messages = [
                {"role": "system", "content": st.session_state.system_prompt}
            ]

            # æ·»åŠ èŠå¤©å†å²
            for msg in st.session_state.messages:
                if msg["role"] != "system":
                    messages.append({"role": msg["role"], "content": msg["content"]})

            # è°ƒç”¨API
            if st.session_state.get("stream", True):
                # æµå¼å“åº”
                response_stream = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    max_tokens=st.session_state.max_tokens,
                    temperature=st.session_state.temperature,
                    top_p=st.session_state.top_p,
                    stream=True
                )

                # æ˜¾ç¤ºæµå¼å“åº”
                for chunk in response_stream:
                    if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                        content_chunk = chunk.choices[0].delta.content
                        full_response += content_chunk
                        message_placeholder.markdown(full_response + "â–Œ")
                        time.sleep(0.01)  # æ·»åŠ å°å»¶è¿Ÿä½¿æµå¼æ•ˆæœæ›´æ˜æ˜¾

                message_placeholder.markdown(full_response)
            else:
                # éæµå¼å“åº”
                response = client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    max_tokens=st.session_state.max_tokens,
                    temperature=st.session_state.temperature,
                    top_p=st.session_state.top_p,
                    stream=False
                )

                full_response = response.choices[0].message.content
                message_placeholder.markdown(full_response)

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
            message_placeholder.markdown(f"âŒ é”™è¯¯: {str(e)}")
